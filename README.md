ðŸ›’ ReviewSentiment Analyzer â€” README

Actionable customerâ€‘feedback insights with fast NLP: clean and label reviews (ruleâ€‘based or ML), visualize patterns, export artifacts (HDF5/PKL/YAML/JSON), and ship instant predictions via Gradio UI or simple scripts.

âœ¨ What this repo gives you

Endâ€‘toâ€‘end pipeline (review_sentiment_analyzer.py):

Ingest CSV/JSON (map your text/rating/id/category columns)

Clean text (HTML/URLs/emoji/punct), lemmatize tokens

Sentiment engines: VADER/TextBlob (no training) or TFâ€‘IDF + Logistic Regression

Visuals: sentiment distribution, Rating Ã— Sentiment heatmap, positive/negative wordclouds

Insights: products with high rating but negative text (hidden issues), top keywords

Gradio UI for instant predictions (single & batch tabs)

Artifacts builder (Windows) (build_artifacts.py):

Reads your SQLite + CSV + hashes.txt

Dedupes via hashes.txt and updates it

Trains ML model & exports: HDF5, PKL, YAML, JSON, eval CSV

Prediction packs:

Notebook utilities (cells): single/batch/file prediction without argparse

CLI script (predict_simple.py): single text or whole file â†’ predictions CSV

All three files are provided in canvas:

Review Sentiment Analyzer â€” Full Pipeline (python + Gradio + Visuals) â†’ review_sentiment_analyzer.py

Review Sentiment â€” Build Artifacts From Sqlite+csv+hashes (windows) â†’ build_artifacts.py

Review Sentiment â€” Simple Prediction Script (CLI + Importable) â†’ predict_simple.py

Review Sentiment Notebook (Jupyter) â€” Build Artifacts & UI (optional, no argparse)

ReviewSentiment â€” Prediction & Results (Notebook Cells + Script) (optional, no argparse)

ðŸ“¦ Requirements

Python 3.11 (tested on Windows 10/11)

Packages (install via pip):

pandas numpy scikit-learn nltk textblob vaderSentiment matplotlib seaborn wordcloud gradio tqdm emoji beautifulsoup4 html5lib pyyaml joblib

(Optional): for BERT fineâ€‘tuning: torch transformers datasets accelerate

Create a virtual environment (Windows PowerShell)
python -m venv .venv
.\.venv\Scripts\activate
pip install -U pip
pip install pandas numpy scikit-learn nltk textblob vaderSentiment matplotlib seaborn wordcloud gradio tqdm emoji beautifulsoup4 html5lib pyyaml joblib

Jupyter users: If you see TqdmWarning: IProgress not found, install widgets:

pip install ipywidgets
ðŸ—‚ï¸ Data & Paths

Default Windows inputs for the artifacts builder:

SQLite : C:\Users\NXTWAVE\Downloads\Review Sentiment Analyzer\archive\database.sqlite
CSV    : C:\Users\NXTWAVE\Downloads\Review Sentiment Analyzer\archive\Reviews.csv
Hashes : C:\Users\NXTWAVE\Downloads\Review Sentiment Analyzer\archive\hashes.txt
Output : C:\Users\NXTWAVE\Downloads\Review Sentiment Analyzer

Typical column names per dataset:

Amazon Fine Food Reviews: Text, Score, ProductId, Summary

Generic eâ€‘commerce: reviewText, rating, productID, productCategory

ðŸš€ Quickstart A â€” Full Pipeline (CSV/JSON â†’ model + visuals + UI)

Script: review_sentiment_analyzer.py

# 1) Oneâ€‘time NLTK download
python review_sentiment_analyzer.py --init-nltk


# 2) Train + analyze on a CSV (map your columns)
python review_sentiment_analyzer.py `
  --data "C:/path/Reviews.csv" `
  --text-col Text `
  --rating-col Score `
  --id-col ProductId `
  --category-col Category `
  --engine ml --do-eda `
  --outdir "C:/path/out"


# 3) Launch Gradio app (uses model if present, else rule-based)
python review_sentiment_analyzer.py --app --outdir "C:/path/out"


# 4) Quick rule-based scoring without training
python review_sentiment_analyzer.py `
  --data "C:/path/Reviews.csv" --text-col Text --rating-col Score `
  --engine rule --predict-only --outdir "C:/path/out"

Key outputs (in --outdir)

model_tfidf_lr.joblib â€” trained TFâ€‘IDF + Logistic Regression model

eval_classification_report.csv, eval_confusion_matrix.csv, eval_predictions_valid.csv

viz_sentiment_distribution.png, viz_rating_x_sentiment_heatmap.png, viz_wordcloud_positive.png, viz_wordcloud_negative.png

top_keywords_positive.csv, top_keywords_negative.csv, products_hidden_issues.csv

Common flags

Flag	Description
--data	Path to CSV or JSON reviews file
--text-col	Column with raw review text
--rating-col	Numeric rating (e.g., Score 1â€“5)
--id-col	Product identifier (e.g., ProductId, asin)
--category-col	Product category name/id
--label-col	If you already have labeled sentiment (positive/negative/neutral)
--engine	ml (default) or rule
--do-eda	Save visuals/insights
--app	Launch Gradio UI
--outdir	Output folder (defaults to ./sentiment_out)
âš™ï¸ Quickstart B â€” Build artifacts from SQLite + CSV + hashes (Windows)

Script: build_artifacts.py

# Runs with the default paths listed above
python build_artifacts.py

Exports to C:\Users\NXTWAVE\Downloads\Review Sentiment Analyzer:

processed_reviews.h5 â€” cleaned & normalized dataset

model_sentiment.pkl â€” trained TFâ€‘IDF + Logistic Regression pipeline

build_metadata.yaml â€” inputs, schema, counts, metrics

insights.json â€” sentiment distribution + hiddenâ€‘issues products

eval_predictions_valid.csv â€” validation predictions

updates archive\hashes.txt with new dedupe hashes

ðŸ”® Quickstart C â€” Predictions (single, batch, or file)
Option 1: Jupyter Notebook

Use the â€œReviewSentiment â€” Prediction & Results (Notebook Cells + Script)â€ file.

# Single / batch in notebook
from pathlib import Path
TEST = [
  "The camera is great but the battery dies too fast",
  "Terrible build quality",
]
res = predict_many(TEST)
res.head()


# File prediction â†’ CSV
file_out = predict_from_file(
    input_path=Path(r"C:\Users\NXTWAVE\Downloads\Review Sentiment Analyzer\archive\Reviews.csv"),
    text_col='Text',
    out_csv=Path(r"C:\Users\NXTWAVE\Downloads\Review Sentiment Analyzer\predictions_from_reviews.csv")
)
Option 2: CLI Script

Script: predict_simple.py

# Single text
python predict_simple.py --model "C:/path/out/model_tfidf_lr.joblib" `
  --text "Battery life is poor but display is gorgeous"


# Batch file â†’ predictions CSV
python predict_simple.py --model "C:/path/out/model_tfidf_lr.joblib" `
  --file  "C:/Users/NXTWAVE/Downloads/Review Sentiment Analyzer/archive/Reviews.csv" `
  --text-col Text `
  --out   "C:/Users/NXTWAVE/Downloads/Review Sentiment Analyzer/predictions_from_reviews.csv"

If --model is omitted, the script will try model_tfidf_lr.joblib beside the script. If no model is found, it falls back to VADER (ruleâ€‘based) predictions.

Option 3: Gradio UI (from Full Pipeline)
python review_sentiment_analyzer.py --app --outdir "C:/path/out"

Open the printed http://127.0.0.1:7860 URL and use the Single or Batch tab.

ðŸ§  How labels are created

Supervised path: If you pass --label-col, the model trains on your labels.

Ratingâ€‘derived: If --rating-col is numeric: >=4 â†’ positive, <=2 â†’ negative, else neutral.

Ruleâ€‘based fallback: VADER/TextBlob polarity mapped to positive/neutral/negative.

If neutral is very scarce (< ~50 samples), the pipeline automatically trains a binary model for stability.

ðŸ“Š Visuals & Insights

Distribution: class balance for quick sanity checks

Rating Ã— Sentiment heatmap: where stars disagree with text sentiment

Wordclouds: separate clouds for positive and negative reviews

Hiddenâ€‘issues products: high average rating but â‰¥15% negative text (â‰¥10 reviews)
![Confusion Matrix Heatmap](viz_wordcloud_positive.png)
ðŸ§© Project layout (suggested)
.
â”œâ”€ review_sentiment_analyzer.py         # full CLI + Gradio
â”œâ”€ build_artifacts.py                   # Windows builder (SQLite+CSV+hashes)
â”œâ”€ predict_simple.py                    # lightweight CLI predictor
â”œâ”€ notebooks/
â”‚  â”œâ”€ ReviewSentiment_Notebook.ipynb    # optional: build & UI without argparse
â”‚  â””â”€ Prediction_Results.ipynb          # optional: notebook prediction pack
â””â”€ outputs/
   â”œâ”€ model_tfidf_lr.joblib
   â”œâ”€ eval_*.csv
   â”œâ”€ viz_*.png
   â”œâ”€ processed_reviews.h5
   â”œâ”€ model_sentiment.pkl
   â”œâ”€ build_metadata.yaml
   â””â”€ insights.json
ðŸ›¡ï¸ Troubleshooting

Argparse error in Jupyter:

ipykernel_launcher.py ... error: unrecognized arguments: -f ...kernel.json

Use the Notebook versions (no CLI flags) provided in canvas.

tqdm IProgress warning in Jupyter:

TqdmWarning: IProgress not found

Install widgets:

pip install ipywidgets

PowerShell canâ€™t activate venv (execution policy):

# Current user scope is usually enough
Set-ExecutionPolicy -Scope CurrentUser RemoteSigned

Then reâ€‘run: .\.venv\Scripts\Activate.ps1

Unicode/HTML in reviews: The cleaner strips emojis/HTML/URLs by default. Tweak clean_text() if you want to keep emojis (for downstream features).

Imbalanced classes: Use --do-eda to inspect class balance. Consider class weights or downsampling if needed.

âš¡ Performance tips

Start with TFâ€‘IDF + Logistic Regression (fast, strong baseline)

Tune --tfidf max_features and ngram_range inside the pipeline

For large corpora, consider subâ€‘sampling for faster iteration

Optional: switch to DistilBERT fineâ€‘tuning if GPU is available (not included by default)

ðŸ” Privacy note

Reviews may contain PII. Mask or drop columns you donâ€™t need before exporting artifacts. Avoid committing raw datasets to public repos.

ðŸ™Œ Credits

VADER (Hutto & Gilbert), TextBlob, scikitâ€‘learn, NLTK, Matplotlib, Seaborn, WordCloud, Gradio

Thanks to open review datasets (e.g., Amazon Reviews, IMDb) for benchmarking
AUTHOR
SAGNIK PATRA
