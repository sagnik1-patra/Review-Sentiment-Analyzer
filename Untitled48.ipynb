{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b5cd6da-e7ae-414d-9f55-97bf4309fd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
      "   ---------------------------------------- 0.0/608.4 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 262.1/608.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 608.4/608.4 kB 1.5 MB/s  0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44868a89-89d2-44ab-b9f0-a3b24ebf8972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.9->textblob) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.9->textblob) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.9->textblob) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "   ---------------------------------------- 0.0/624.3 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/624.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 624.3/624.3 kB 1.4 MB/s  0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8b34f00-341f-4f37-b996-0c72d610e3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
      "Requirement already satisfied: requests in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from vaderSentiment) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->vaderSentiment) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->vaderSentiment) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->vaderSentiment) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->vaderSentiment) (2025.8.3)\n",
      "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "286a7bd3-88cb-44af-8c45-b1299629b1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp311-cp311-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wordcloud) (11.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wordcloud) (3.10.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (3.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
      "Downloading wordcloud-1.9.4-cp311-cp311-win_amd64.whl (299 kB)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7308ac9-22e3-4fa2-bc27-f8d2bab46314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-5.47.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (4.10.0)\n",
      "Collecting brotli>=1.1.0 (from gradio)\n",
      "  Downloading Brotli-1.1.0-cp311-cp311-win_amd64.whl.metadata (5.6 kB)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.117.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.6.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.13.2 (from gradio)\n",
      "  Downloading gradio_client-1.13.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Collecting huggingface-hub<1.0,>=0.33.5 (from gradio)\n",
      "  Downloading huggingface_hub-0.35.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (1.26.4)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.11.3-cp311-cp311-win_amd64.whl.metadata (43 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (2.3.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (11.3.0)\n",
      "Collecting pydantic<2.12,>=2.0 (from gradio)\n",
      "  Downloading pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (6.0.2)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.13.1-py3-none-win_amd64.whl.metadata (26 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.48.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading typer-0.19.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (4.15.0)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.37.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting fsspec (from gradio-client==1.13.2->gradio)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting websockets<16.0,>=13.0 (from gradio-client==1.13.2->gradio)\n",
      "  Downloading websockets-15.0.1-cp311-cp311-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.33.5->gradio)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<2.12,>=2.0->gradio)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<2.12,>=2.0->gradio)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<2.12,>=2.0->gradio)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nxtwave\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.5.0)\n",
      "Downloading gradio-5.47.0-py3-none-any.whl (60.4 MB)\n",
      "   ---------------------------------------- 0.0/60.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/60.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.8/60.4 MB 2.0 MB/s eta 0:00:31\n",
      "    --------------------------------------- 1.0/60.4 MB 2.0 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 1.6/60.4 MB 2.0 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 1.8/60.4 MB 2.0 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 2.4/60.4 MB 2.0 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 2.9/60.4 MB 2.0 MB/s eta 0:00:30\n",
      "   -- ------------------------------------- 3.1/60.4 MB 2.0 MB/s eta 0:00:30\n",
      "   -- ------------------------------------- 3.7/60.4 MB 2.0 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 3.9/60.4 MB 2.0 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 4.5/60.4 MB 2.0 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 4.7/60.4 MB 2.0 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 5.2/60.4 MB 2.0 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 5.5/60.4 MB 2.0 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 6.0/60.4 MB 2.0 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 6.3/60.4 MB 2.0 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 6.8/60.4 MB 2.0 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 7.1/60.4 MB 1.9 MB/s eta 0:00:28\n",
      "   ----- ---------------------------------- 7.6/60.4 MB 1.9 MB/s eta 0:00:28\n",
      "   ----- ---------------------------------- 7.9/60.4 MB 1.9 MB/s eta 0:00:28\n",
      "   ----- ---------------------------------- 8.4/60.4 MB 1.9 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 8.7/60.4 MB 1.9 MB/s eta 0:00:27\n",
      "   ------ --------------------------------- 9.2/60.4 MB 1.9 MB/s eta 0:00:27\n",
      "   ------ --------------------------------- 9.4/60.4 MB 1.9 MB/s eta 0:00:27\n",
      "   ------ --------------------------------- 10.0/60.4 MB 1.9 MB/s eta 0:00:27\n",
      "   ------ --------------------------------- 10.2/60.4 MB 1.9 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 10.7/60.4 MB 1.9 MB/s eta 0:00:26\n",
      "   ------- -------------------------------- 11.3/60.4 MB 1.9 MB/s eta 0:00:26\n",
      "   ------- -------------------------------- 11.5/60.4 MB 1.9 MB/s eta 0:00:26\n",
      "   ------- -------------------------------- 12.1/60.4 MB 1.9 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 12.3/60.4 MB 1.9 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 12.6/60.4 MB 1.9 MB/s eta 0:00:25\n",
      "   -------- ------------------------------- 13.1/60.4 MB 1.9 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 13.6/60.4 MB 1.9 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 13.9/60.4 MB 1.9 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 14.4/60.4 MB 1.9 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 14.7/60.4 MB 1.9 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 15.2/60.4 MB 1.9 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 15.5/60.4 MB 1.9 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 16.0/60.4 MB 1.9 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 16.3/60.4 MB 1.9 MB/s eta 0:00:23\n",
      "   ----------- ---------------------------- 16.8/60.4 MB 1.9 MB/s eta 0:00:23\n",
      "   ----------- ---------------------------- 17.0/60.4 MB 1.9 MB/s eta 0:00:23\n",
      "   ----------- ---------------------------- 17.0/60.4 MB 1.9 MB/s eta 0:00:23\n",
      "   ----------- ---------------------------- 17.3/60.4 MB 1.9 MB/s eta 0:00:24\n",
      "   ----------- ---------------------------- 17.6/60.4 MB 1.8 MB/s eta 0:00:24\n",
      "   ----------- ---------------------------- 17.6/60.4 MB 1.8 MB/s eta 0:00:24\n",
      "   ----------- ---------------------------- 17.8/60.4 MB 1.8 MB/s eta 0:00:24\n",
      "   ----------- ---------------------------- 17.8/60.4 MB 1.8 MB/s eta 0:00:24\n",
      "   ----------- ---------------------------- 18.1/60.4 MB 1.7 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 18.4/60.4 MB 1.7 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 18.6/60.4 MB 1.7 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 18.9/60.4 MB 1.7 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 18.9/60.4 MB 1.7 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 19.1/60.4 MB 1.7 MB/s eta 0:00:25\n",
      "   ------------- -------------------------- 19.7/60.4 MB 1.7 MB/s eta 0:00:25\n",
      "   ------------- -------------------------- 19.9/60.4 MB 1.7 MB/s eta 0:00:25\n",
      "   ------------- -------------------------- 20.4/60.4 MB 1.7 MB/s eta 0:00:24\n",
      "   ------------- -------------------------- 20.7/60.4 MB 1.7 MB/s eta 0:00:24\n",
      "   ------------- -------------------------- 21.0/60.4 MB 1.7 MB/s eta 0:00:24\n",
      "   -------------- ------------------------- 21.2/60.4 MB 1.7 MB/s eta 0:00:24\n",
      "   -------------- ------------------------- 21.5/60.4 MB 1.7 MB/s eta 0:00:24\n",
      "   -------------- ------------------------- 21.8/60.4 MB 1.7 MB/s eta 0:00:24\n",
      "   -------------- ------------------------- 22.0/60.4 MB 1.7 MB/s eta 0:00:24\n",
      "   -------------- ------------------------- 22.5/60.4 MB 1.7 MB/s eta 0:00:23\n",
      "   -------------- ------------------------- 22.5/60.4 MB 1.7 MB/s eta 0:00:23\n",
      "   --------------- ------------------------ 22.8/60.4 MB 1.6 MB/s eta 0:00:23\n",
      "   --------------- ------------------------ 23.1/60.4 MB 1.6 MB/s eta 0:00:23\n",
      "   --------------- ------------------------ 23.1/60.4 MB 1.6 MB/s eta 0:00:23\n",
      "   --------------- ------------------------ 23.3/60.4 MB 1.6 MB/s eta 0:00:24\n",
      "   --------------- ------------------------ 23.6/60.4 MB 1.6 MB/s eta 0:00:24\n",
      "   --------------- ------------------------ 23.6/60.4 MB 1.6 MB/s eta 0:00:24\n",
      "   --------------- ------------------------ 23.9/60.4 MB 1.6 MB/s eta 0:00:24\n",
      "   --------------- ------------------------ 24.1/60.4 MB 1.6 MB/s eta 0:00:24\n",
      "   ---------------- ----------------------- 24.4/60.4 MB 1.6 MB/s eta 0:00:24\n",
      "   ---------------- ----------------------- 24.6/60.4 MB 1.6 MB/s eta 0:00:23\n",
      "   ---------------- ----------------------- 24.9/60.4 MB 1.6 MB/s eta 0:00:23\n",
      "   ---------------- ----------------------- 25.2/60.4 MB 1.6 MB/s eta 0:00:23\n",
      "   ---------------- ----------------------- 25.4/60.4 MB 1.6 MB/s eta 0:00:23\n",
      "   ----------------- ---------------------- 26.0/60.4 MB 1.6 MB/s eta 0:00:23\n",
      "   ----------------- ---------------------- 26.2/60.4 MB 1.6 MB/s eta 0:00:22\n",
      "   ----------------- ---------------------- 26.7/60.4 MB 1.6 MB/s eta 0:00:22\n",
      "   ----------------- ---------------------- 27.0/60.4 MB 1.6 MB/s eta 0:00:22\n",
      "   ------------------ --------------------- 27.3/60.4 MB 1.6 MB/s eta 0:00:22\n",
      "   ------------------ --------------------- 27.5/60.4 MB 1.6 MB/s eta 0:00:22\n",
      "   ------------------ --------------------- 28.0/60.4 MB 1.6 MB/s eta 0:00:21\n",
      "   ------------------ --------------------- 28.3/60.4 MB 1.6 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 28.8/60.4 MB 1.6 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 29.1/60.4 MB 1.6 MB/s eta 0:00:20\n",
      "   ------------------- -------------------- 29.4/60.4 MB 1.6 MB/s eta 0:00:20\n",
      "   ------------------- -------------------- 29.4/60.4 MB 1.6 MB/s eta 0:00:20\n",
      "   ------------------- -------------------- 29.6/60.4 MB 1.6 MB/s eta 0:00:20\n",
      "   ------------------- -------------------- 29.9/60.4 MB 1.5 MB/s eta 0:00:20\n",
      "   ------------------- -------------------- 30.1/60.4 MB 1.5 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 30.4/60.4 MB 1.5 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 30.7/60.4 MB 1.5 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 30.9/60.4 MB 1.5 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 31.2/60.4 MB 1.5 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 31.7/60.4 MB 1.5 MB/s eta 0:00:19\n",
      "   --------------------- ------------------ 32.0/60.4 MB 1.5 MB/s eta 0:00:19\n",
      "   --------------------- ------------------ 32.2/60.4 MB 1.5 MB/s eta 0:00:19\n",
      "   --------------------- ------------------ 32.5/60.4 MB 1.5 MB/s eta 0:00:19\n",
      "   --------------------- ------------------ 33.0/60.4 MB 1.5 MB/s eta 0:00:18\n",
      "   ---------------------- ----------------- 33.3/60.4 MB 1.5 MB/s eta 0:00:18\n",
      "   ---------------------- ----------------- 33.6/60.4 MB 1.5 MB/s eta 0:00:18\n",
      "   ---------------------- ----------------- 34.1/60.4 MB 1.5 MB/s eta 0:00:18\n",
      "   ---------------------- ----------------- 34.1/60.4 MB 1.5 MB/s eta 0:00:18\n",
      "   ---------------------- ----------------- 34.6/60.4 MB 1.5 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 35.1/60.4 MB 1.5 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 35.4/60.4 MB 1.5 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 35.7/60.4 MB 1.5 MB/s eta 0:00:16\n",
      "   ----------------------- ---------------- 35.9/60.4 MB 1.5 MB/s eta 0:00:16\n",
      "   ----------------------- ---------------- 35.9/60.4 MB 1.5 MB/s eta 0:00:16\n",
      "   ----------------------- ---------------- 36.2/60.4 MB 1.5 MB/s eta 0:00:16\n",
      "   ------------------------ --------------- 36.4/60.4 MB 1.5 MB/s eta 0:00:16\n",
      "   ------------------------ --------------- 36.7/60.4 MB 1.5 MB/s eta 0:00:16\n",
      "   ------------------------ --------------- 37.2/60.4 MB 1.5 MB/s eta 0:00:16\n",
      "   ------------------------ --------------- 37.5/60.4 MB 1.5 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 38.0/60.4 MB 1.5 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 38.3/60.4 MB 1.5 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 38.8/60.4 MB 1.5 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 38.8/60.4 MB 1.5 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 39.3/60.4 MB 1.5 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 39.6/60.4 MB 1.5 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 39.8/60.4 MB 1.5 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 40.1/60.4 MB 1.5 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 40.4/60.4 MB 1.5 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 40.6/60.4 MB 1.5 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 41.2/60.4 MB 1.5 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 41.2/60.4 MB 1.5 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 41.4/60.4 MB 1.5 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 41.4/60.4 MB 1.5 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 41.4/60.4 MB 1.5 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 41.7/60.4 MB 1.5 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 42.2/60.4 MB 1.5 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 42.5/60.4 MB 1.5 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 43.0/60.4 MB 1.5 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 43.0/60.4 MB 1.5 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 43.5/60.4 MB 1.5 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 43.8/60.4 MB 1.5 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 44.3/60.4 MB 1.5 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 44.6/60.4 MB 1.5 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 44.8/60.4 MB 1.5 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 45.1/60.4 MB 1.5 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 45.4/60.4 MB 1.5 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 45.6/60.4 MB 1.5 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 46.1/60.4 MB 1.5 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 46.4/60.4 MB 1.5 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 46.7/60.4 MB 1.5 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 46.9/60.4 MB 1.5 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 47.2/60.4 MB 1.5 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 47.2/60.4 MB 1.5 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 47.4/60.4 MB 1.5 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 48.0/60.4 MB 1.4 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 48.0/60.4 MB 1.4 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 48.2/60.4 MB 1.4 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 48.5/60.4 MB 1.4 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 48.8/60.4 MB 1.4 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 49.0/60.4 MB 1.4 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 49.5/60.4 MB 1.4 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 49.8/60.4 MB 1.4 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 49.8/60.4 MB 1.4 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 50.1/60.4 MB 1.4 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 50.3/60.4 MB 1.4 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 50.6/60.4 MB 1.4 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 50.9/60.4 MB 1.4 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 51.1/60.4 MB 1.4 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 51.4/60.4 MB 1.4 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 51.9/60.4 MB 1.4 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 52.2/60.4 MB 1.4 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 52.4/60.4 MB 1.4 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 52.7/60.4 MB 1.4 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 53.2/60.4 MB 1.4 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 53.5/60.4 MB 1.4 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 53.7/60.4 MB 1.4 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 54.3/60.4 MB 1.4 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 54.5/60.4 MB 1.4 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 54.8/60.4 MB 1.4 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 55.1/60.4 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 55.3/60.4 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 55.6/60.4 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 55.8/60.4 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 56.1/60.4 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 56.6/60.4 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 56.9/60.4 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 57.1/60.4 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 57.7/60.4 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 57.9/60.4 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 58.2/60.4 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 58.5/60.4 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  59.0/60.4 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  59.2/60.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  59.5/60.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  60.0/60.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  60.3/60.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 60.4/60.4 MB 1.4 MB/s  0:00:41\n",
      "Downloading gradio_client-1.13.2-py3-none-any.whl (325 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading fastapi-0.117.1-py3-none-any.whl (95 kB)\n",
      "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n",
      "   ---------------------------------------- 0.0/563.3 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 262.1/563.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 563.3/563.3 kB 1.3 MB/s  0:00:00\n",
      "Downloading orjson-3.11.3-cp311-cp311-win_amd64.whl (131 kB)\n",
      "Downloading pydantic-2.11.9-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 989.2 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 989.2 kB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 838.9 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.0/2.0 MB 898.8 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.3/2.0 MB 945.5 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/2.0 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 1.1 MB/s  0:00:01\n",
      "Downloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.48.0-py3-none-any.whl (73 kB)\n",
      "Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Downloading typer-0.19.2-py3-none-any.whl (46 kB)\n",
      "Downloading websockets-15.0.1-cp311-cp311-win_amd64.whl (176 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading Brotli-1.1.0-cp311-cp311-win_amd64.whl (357 kB)\n",
      "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading ruff-0.13.1-py3-none-win_amd64.whl (13.3 MB)\n",
      "   ---------------------------------------- 0.0/13.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/13.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/13.3 MB 989.2 kB/s eta 0:00:13\n",
      "   -- ------------------------------------- 0.8/13.3 MB 1.1 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.0/13.3 MB 1.1 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.3/13.3 MB 1.1 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 1.6/13.3 MB 1.1 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 1.8/13.3 MB 1.2 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 2.1/13.3 MB 1.2 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 2.6/13.3 MB 1.3 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 2.9/13.3 MB 1.3 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 3.1/13.3 MB 1.3 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 3.1/13.3 MB 1.3 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 3.7/13.3 MB 1.3 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 3.9/13.3 MB 1.3 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 4.5/13.3 MB 1.3 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 4.7/13.3 MB 1.4 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 5.0/13.3 MB 1.4 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 5.5/13.3 MB 1.4 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 5.8/13.3 MB 1.4 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 6.0/13.3 MB 1.4 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 6.6/13.3 MB 1.5 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 6.8/13.3 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 7.3/13.3 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 7.6/13.3 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 8.1/13.3 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 8.4/13.3 MB 1.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 8.7/13.3 MB 1.5 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 9.2/13.3 MB 1.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 9.4/13.3 MB 1.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 9.7/13.3 MB 1.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 10.0/13.3 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 10.5/13.3 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 10.7/13.3 MB 1.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.0/13.3 MB 1.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.3/13.3 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.5/13.3 MB 1.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 11.8/13.3 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.1/13.3 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.6/13.3 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.8/13.3 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.3 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.3/13.3 MB 1.5 MB/s  0:00:09\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading uvicorn-0.37.0-py3-none-any.whl (67 kB)\n",
      "Downloading ffmpy-0.6.1-py3-none-any.whl (5.5 kB)\n",
      "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub, brotli, websockets, typing-inspection, tomlkit, shellingham, semantic-version, ruff, python-multipart, pydantic-core, orjson, mdurl, groovy, fsspec, filelock, ffmpy, annotated-types, aiofiles, uvicorn, starlette, pydantic, markdown-it-py, huggingface-hub, safehttpx, rich, gradio-client, fastapi, typer, gradio\n",
      "\n",
      "   -- -------------------------------------  2/29 [websockets]\n",
      "   -- -------------------------------------  2/29 [websockets]\n",
      "   ---- -----------------------------------  3/29 [typing-inspection]\n",
      "   ------ ---------------------------------  5/29 [shellingham]\n",
      "   --------- ------------------------------  7/29 [ruff]\n",
      "   --------- ------------------------------  7/29 [ruff]\n",
      "   ------------ ---------------------------  9/29 [pydantic-core]\n",
      "   ----------------- ---------------------- 13/29 [fsspec]\n",
      "   ----------------- ---------------------- 13/29 [fsspec]\n",
      "   ------------------- -------------------- 14/29 [filelock]\n",
      "   ----------------------- ---------------- 17/29 [aiofiles]\n",
      "   ------------------------ --------------- 18/29 [uvicorn]\n",
      "   -------------------------- ------------- 19/29 [starlette]\n",
      "   --------------------------- ------------ 20/29 [pydantic]\n",
      "   --------------------------- ------------ 20/29 [pydantic]\n",
      "   --------------------------- ------------ 20/29 [pydantic]\n",
      "   --------------------------- ------------ 20/29 [pydantic]\n",
      "   --------------------------- ------------ 20/29 [pydantic]\n",
      "   ---------------------------- ----------- 21/29 [markdown-it-py]\n",
      "   ---------------------------- ----------- 21/29 [markdown-it-py]\n",
      "   ------------------------------ --------- 22/29 [huggingface-hub]\n",
      "   ------------------------------ --------- 22/29 [huggingface-hub]\n",
      "   ------------------------------ --------- 22/29 [huggingface-hub]\n",
      "   ------------------------------ --------- 22/29 [huggingface-hub]\n",
      "   ------------------------------ --------- 22/29 [huggingface-hub]\n",
      "   ------------------------------ --------- 22/29 [huggingface-hub]\n",
      "   --------------------------------- ------ 24/29 [rich]\n",
      "   --------------------------------- ------ 24/29 [rich]\n",
      "   --------------------------------- ------ 24/29 [rich]\n",
      "   ---------------------------------- ----- 25/29 [gradio-client]\n",
      "   ----------------------------------- ---- 26/29 [fastapi]\n",
      "   ----------------------------------- ---- 26/29 [fastapi]\n",
      "   ------------------------------------- -- 27/29 [typer]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   -------------------------------------- - 28/29 [gradio]\n",
      "   ---------------------------------------- 29/29 [gradio]\n",
      "\n",
      "Successfully installed aiofiles-24.1.0 annotated-types-0.7.0 brotli-1.1.0 fastapi-0.117.1 ffmpy-0.6.1 filelock-3.19.1 fsspec-2025.9.0 gradio-5.47.0 gradio-client-1.13.2 groovy-0.1.2 huggingface-hub-0.35.1 markdown-it-py-4.0.0 mdurl-0.1.2 orjson-3.11.3 pydantic-2.11.9 pydantic-core-2.33.2 pydub-0.25.1 python-multipart-0.0.20 rich-14.1.0 ruff-0.13.1 safehttpx-0.1.6 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.48.0 tomlkit-0.13.3 typer-0.19.2 typing-inspection-0.4.1 uvicorn-0.37.0 websockets-15.0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "020be292-69dd-412c-a19f-fac8e9f0f7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "usage: ipykernel_launcher.py [-h] [--data DATA] [--text-col TEXT_COL] [--rating-col RATING_COL] [--id-col ID_COL]\n",
      "                             [--category-col CATEGORY_COL] [--label-col LABEL_COL] [--engine {rule,ml}] [--predict-only] [--do-eda]\n",
      "                             [--outdir OUTDIR] [--seed SEED] [--app] [--init-nltk]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\NXTWAVE\\AppData\\Roaming\\jupyter\\runtime\\kernel-0d5d4d2f-5910-4440-8d3b-68d412d0efa7.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, re, json, math, argparse, random, string, warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import emoji as emoji_lib\n",
    "\n",
    "# Sentiment toolkits\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# UI\n",
    "import gradio as gr\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -------------- Utils --------------\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)\n",
    "HTML_TAG_RE = re.compile(r\"<[^>]+>\")\n",
    "NON_ASCII_RE = re.compile(r\"[^\\x00-\\x7F]+\")\n",
    "PUNCT_TABLE = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def ensure_dir(p: str | Path) -> Path:\n",
    "    p = Path(p)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    t = text\n",
    "    # HTML unescape & strip tags\n",
    "    t = BeautifulSoup(t, \"html5lib\").get_text(\" \")\n",
    "    # URLs\n",
    "    t = URL_RE.sub(\" \", t)\n",
    "    # emoji -> text alias (optional)\n",
    "    t = emoji_lib.demojize(t, delimiters=(\" \", \" \"))\n",
    "    # lower\n",
    "    t = t.lower()\n",
    "    # remove punctuation\n",
    "    t = t.translate(PUNCT_TABLE)\n",
    "    # remove non-ascii\n",
    "    t = NON_ASCII_RE.sub(\" \", t)\n",
    "    # collapse whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "\n",
    "def tokenize_and_lemmatize(t: str, stop_words: set[str]) -> List[str]:\n",
    "    toks = [lemmatizer.lemmatize(w) for w in t.split() if w not in stop_words and len(w) > 2]\n",
    "    return toks\n",
    "\n",
    "\n",
    "def rule_sentiment_score(text: str, use_vader=True, use_textblob=True) -> float:\n",
    "    \"\"\"Return compound polarity in [-1,1]. Combines VADER + TextBlob by averaging when both used.\"\"\"\n",
    "    scores = []\n",
    "    if use_vader:\n",
    "        vs = SentimentIntensityAnalyzer()\n",
    "        scores.append(vs.polarity_scores(text).get('compound', 0.0))\n",
    "    if use_textblob:\n",
    "        try:\n",
    "            scores.append(TextBlob(text).sentiment.polarity)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not scores:\n",
    "        return 0.0\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "\n",
    "def score_to_label(score: float, pos=0.2, neg=-0.2) -> str:\n",
    "    if score >= pos:\n",
    "        return \"positive\"\n",
    "    if score <= neg:\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "\n",
    "# -------------- Data Loading --------------\n",
    "\n",
    "def load_dataset(path: str | Path, text_col: str, rating_col: Optional[str] = None,\n",
    "                 id_col: Optional[str] = None, category_col: Optional[str] = None,\n",
    "                 label_col: Optional[str] = None) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Data not found: {path}\")\n",
    "    if path.suffix.lower() == \".json\":\n",
    "        df = pd.read_json(path, lines=True)\n",
    "    else:\n",
    "        df = pd.read_csv(path)\n",
    "    # Normalize column names for safer matching\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # Minimal schema\n",
    "    req = [text_col]\n",
    "    for c in req:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"Column '{c}' not found. Available: {df.columns.tolist()}\")\n",
    "\n",
    "    keep = {text_col: 'reviewText'}\n",
    "    if rating_col and rating_col in df.columns:\n",
    "        keep[rating_col] = 'rating'\n",
    "    if id_col and id_col in df.columns:\n",
    "        keep[id_col] = 'productID'\n",
    "    if category_col and category_col in df.columns:\n",
    "        keep[category_col] = 'productCategory'\n",
    "    if label_col and label_col in df.columns:\n",
    "        keep[label_col] = 'label'\n",
    "\n",
    "    out = df[list(keep.keys())].rename(columns=keep)\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------- Training --------------\n",
    "\n",
    "def build_ml_pipeline(max_features=50000, ngram_range=(1,2), C=4.0) -> Pipeline:\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)),\n",
    "        (\"clf\", LogisticRegression(C=C, max_iter=200, n_jobs=-1)),\n",
    "    ])\n",
    "\n",
    "\n",
    "def prepare_labels_for_ml(df: pd.DataFrame, rating_as_label: bool = True) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if 'label' in out.columns and not rating_as_label:\n",
    "        # Expect string labels like 'positive'/'negative'\n",
    "        return out\n",
    "    # Construct label from rating if present else fallback via rule-based score\n",
    "    if 'rating' in out.columns and pd.api.types.is_numeric_dtype(out['rating']):\n",
    "        out['label'] = out['rating'].apply(lambda r: 'positive' if r >= 4 else ('negative' if r <= 2 else 'neutral'))\n",
    "    else:\n",
    "        out['label'] = out['reviewText'].fillna(\"\").apply(lambda t: score_to_label(rule_sentiment_score(t)))\n",
    "    return out\n",
    "\n",
    "\n",
    "def train_ml(df: pd.DataFrame, outdir: Path, test_size=0.2, seed=42) -> Dict:\n",
    "    df = df.dropna(subset=['reviewText']).copy()\n",
    "    df = prepare_labels_for_ml(df)\n",
    "    # Drop neutral to make a stricter binary classifier if too few neutrals\n",
    "    label_counts = df['label'].value_counts()\n",
    "    if label_counts.get('neutral', 0) < 50:\n",
    "        df = df[df['label'] != 'neutral']\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        df['reviewText'], df['label'], test_size=test_size, random_state=seed, stratify=df['label'])\n",
    "\n",
    "    pipe = build_ml_pipeline()\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipe.predict(X_valid)\n",
    "    y_proba = None\n",
    "    try:\n",
    "        y_proba = pipe.predict_proba(X_valid)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    report = classification_report(y_valid, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_valid, y_pred, labels=sorted(df['label'].unique()))\n",
    "\n",
    "    # Save artifacts\n",
    "    ensure_dir(outdir)\n",
    "    import joblib\n",
    "    joblib.dump(pipe, outdir / 'model_tfidf_lr.joblib')\n",
    "\n",
    "    # Save evals\n",
    "    pd.DataFrame(report).to_csv(outdir / 'eval_classification_report.csv')\n",
    "    pd.DataFrame(cm, index=sorted(df['label'].unique()), columns=sorted(df['label'].unique())).to_csv(outdir / 'eval_confusion_matrix.csv')\n",
    "\n",
    "    return {\n",
    "        'pipe': pipe,\n",
    "        'report': report,\n",
    "        'cm': cm,\n",
    "        'labels': sorted(df['label'].unique()),\n",
    "        'X_valid': X_valid.reset_index(drop=True),\n",
    "        'y_valid': y_valid.reset_index(drop=True),\n",
    "        'y_pred': pd.Series(y_pred),\n",
    "        'y_proba': y_proba\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------- EDA & Visuals --------------\n",
    "\n",
    "def make_distribution_plot(df: pd.DataFrame, outdir: Path):\n",
    "    plt.figure()\n",
    "    ax = sns.countplot(x='label', data=df, order=['negative','neutral','positive'])\n",
    "    ax.set_title('Sentiment distribution')\n",
    "    for c in ax.containers:\n",
    "        ax.bar_label(c)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / 'viz_sentiment_distribution.png', dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def make_rating_vs_sentiment_heatmap(df: pd.DataFrame, outdir: Path):\n",
    "    if 'rating' not in df.columns:\n",
    "        return\n",
    "    tmp = df.copy()\n",
    "    tmp['rating'] = pd.to_numeric(tmp['rating'], errors='coerce')\n",
    "    tmp = tmp.dropna(subset=['rating','label'])\n",
    "    pivot = pd.crosstab(tmp['rating'], tmp['label'])\n",
    "    if pivot.empty:\n",
    "        return\n",
    "    plt.figure()\n",
    "    sns.heatmap(pivot, annot=True, fmt='d')\n",
    "    plt.title('Rating  Sentiment')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / 'viz_rating_x_sentiment_heatmap.png', dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def make_wordclouds(df: pd.DataFrame, outdir: Path):\n",
    "    texts = {\n",
    "        'positive': ' '.join(df[df['label']=='positive']['reviewText'].astype(str).tolist())[:3_000_000],\n",
    "        'negative': ' '.join(df[df['label']=='negative']['reviewText'].astype(str).tolist())[:3_000_000],\n",
    "    }\n",
    "    for lab, txt in texts.items():\n",
    "        if not txt:\n",
    "            continue\n",
    "        wc = WordCloud(width=1200, height=600, background_color='white').generate(txt)\n",
    "        wc.to_file(str(outdir / f'viz_wordcloud_{lab}.png'))\n",
    "\n",
    "\n",
    "def top_keywords(df: pd.DataFrame, label: str, k: int = 20) -> List[Tuple[str,int]]:\n",
    "    stop = set(stopwords.words('english'))\n",
    "    words = []\n",
    "    for t in df[df['label']==label]['reviewText'].dropna():\n",
    "        t = clean_text(t)\n",
    "        words.extend([w for w in t.split() if w not in stop and len(w)>2 and not w.isdigit()])\n",
    "    vc = pd.Series(words).value_counts().head(k)\n",
    "    return list(vc.items())\n",
    "\n",
    "\n",
    "def insight_products_with_hidden_issues(df: pd.DataFrame, rating_threshold=4) -> pd.DataFrame:\n",
    "    # High average rating but with meaningful fraction of negative labels\n",
    "    if 'productID' not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "    agg = df.groupby('productID').agg(\n",
    "        avg_rating=('rating','mean'),\n",
    "        n_reviews=('reviewText','count'),\n",
    "        neg_frac=('label', lambda s: (s=='negative').mean())\n",
    "    ).reset_index()\n",
    "    out = agg[(agg['avg_rating']>=rating_threshold) & (agg['neg_frac']>=0.15) & (agg['n_reviews']>=10)]\n",
    "    return out.sort_values(['neg_frac','n_reviews'], ascending=[False,False]).head(20)\n",
    "\n",
    "\n",
    "# -------------- Prediction helpers --------------\n",
    "\n",
    "def predict_rule(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    vs = SentimentIntensityAnalyzer()\n",
    "    rows = []\n",
    "    for t in tqdm(df['reviewText'].fillna(\"\"), desc='Rule sentiment'):\n",
    "        comp = vs.polarity_scores(t)['compound']\n",
    "        lab = score_to_label(comp)\n",
    "        rows.append((comp, lab))\n",
    "    out = df.copy()\n",
    "    out['score'] = [r[0] for r in rows]\n",
    "    out['label'] = [r[1] for r in rows]\n",
    "    return out\n",
    "\n",
    "\n",
    "def predict_ml(pipe: Pipeline, texts: List[str]) -> Tuple[List[str], Optional[np.ndarray]]:\n",
    "    try:\n",
    "        proba = pipe.predict_proba(texts)\n",
    "        idx = np.argmax(proba, axis=1)\n",
    "        labels = pipe.classes_[idx]\n",
    "        return labels.tolist(), proba\n",
    "    except Exception:\n",
    "        labels = pipe.predict(texts)\n",
    "        return labels.tolist(), None\n",
    "\n",
    "\n",
    "# -------------- Gradio UI --------------\n",
    "\n",
    "def launch_app(outdir: Path):\n",
    "    import joblib\n",
    "    pipe = None\n",
    "    model_path = outdir / 'model_tfidf_lr.joblib'\n",
    "    if model_path.exists():\n",
    "        pipe = joblib.load(model_path)\n",
    "\n",
    "    def _predict_one(text):\n",
    "        if not text or not text.strip():\n",
    "            return \"\", 0.0\n",
    "        if pipe is None:\n",
    "            comp = rule_sentiment_score(text)\n",
    "            return score_to_label(comp), float(comp)\n",
    "        else:\n",
    "            lab, proba = predict_ml(pipe, [text])\n",
    "            conf = 0.0\n",
    "            if proba is not None:\n",
    "                conf = float(np.max(proba))\n",
    "            return lab[0], conf\n",
    "\n",
    "    def _predict_batch(texts):\n",
    "        items = [t.strip() for t in texts.split('\\n') if t.strip()]\n",
    "        if not items:\n",
    "            return \"\"\n",
    "        out = []\n",
    "        for t in items:\n",
    "            lab, score = _predict_one(t)\n",
    "            out.append({\"text\": t, \"sentiment\": lab, \"confidence\": score})\n",
    "        return pd.DataFrame(out)\n",
    "\n",
    "    with gr.Blocks(title=\"ReviewSentiment Analyzer\") as demo:\n",
    "        gr.Markdown(\"#  ReviewSentiment Analyzer\\nEnter a review to classify sentiment.\")\n",
    "        with gr.Tab(\"Single\"):\n",
    "            inp = gr.Textbox(label=\"Review text\", lines=4)\n",
    "            btn = gr.Button(\"Predict\")\n",
    "            out_lab = gr.Label(num_top_classes=3, label=\"Sentiment (top)\"\n",
    "                               ) if pipe is not None else gr.Textbox(label=\"Sentiment\")\n",
    "            out_conf = gr.Number(label=\"Confidence / Score\")\n",
    "            btn.click(_predict_one, inputs=inp, outputs=[out_lab, out_conf])\n",
    "        with gr.Tab(\"Batch\"):\n",
    "            tb = gr.Textbox(label=\"One review per line\", lines=8)\n",
    "            btnb = gr.Button(\"Predict batch\")\n",
    "            outdf = gr.Dataframe(interactive=False)\n",
    "            btnb.click(_predict_batch, inputs=tb, outputs=outdf)\n",
    "        gr.Markdown(\"Model: \" + (\"TFIDF + LogisticRegression\" if pipe is not None else \"Rulebased (VADER/TextBlob)\"))\n",
    "    demo.launch(server_name=\"0.0.0.0\", server_port=7860, show_error=True)\n",
    "\n",
    "\n",
    "# -------------- Main CLI --------------\n",
    "\n",
    "def init_nltk():\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "\n",
    "\n",
    "def do_eda(df: pd.DataFrame, outdir: Path):\n",
    "    make_distribution_plot(df, outdir)\n",
    "    make_rating_vs_sentiment_heatmap(df, outdir)\n",
    "    make_wordclouds(df, outdir)\n",
    "\n",
    "    # Keyword tables\n",
    "    pos_kw = top_keywords(df, 'positive', 25)\n",
    "    neg_kw = top_keywords(df, 'negative', 25)\n",
    "    pd.DataFrame(pos_kw, columns=['keyword','count']).to_csv(outdir / 'top_keywords_positive.csv', index=False)\n",
    "    pd.DataFrame(neg_kw, columns=['keyword','count']).to_csv(outdir / 'top_keywords_negative.csv', index=False)\n",
    "\n",
    "    # Hidden issues\n",
    "    hidden = insight_products_with_hidden_issues(df)\n",
    "    if not hidden.empty:\n",
    "        hidden.to_csv(outdir / 'products_hidden_issues.csv', index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument('--data', type=str, help='Path to CSV/JSON with reviews')\n",
    "    ap.add_argument('--text-col', type=str, default='reviewText')\n",
    "    ap.add_argument('--rating-col', type=str, default=None)\n",
    "    ap.add_argument('--id-col', type=str, default=None)\n",
    "    ap.add_argument('--category-col', type=str, default=None)\n",
    "    ap.add_argument('--label-col', type=str, default=None, help='If provided, supervised labels in {positive,negative,neutral}')\n",
    "\n",
    "    ap.add_argument('--engine', choices=['rule','ml'], default='ml')\n",
    "    ap.add_argument('--predict-only', action='store_true', help='Skip training; just score with rule engine')\n",
    "    ap.add_argument('--do-eda', action='store_true')\n",
    "    ap.add_argument('--outdir', type=str, default='./sentiment_out')\n",
    "    ap.add_argument('--seed', type=int, default=42)\n",
    "    ap.add_argument('--app', action='store_true', help='Launch Gradio app')\n",
    "    ap.add_argument('--init-nltk', action='store_true')\n",
    "\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    outdir = ensure_dir(args.outdir)\n",
    "\n",
    "    if args.init_nltk:\n",
    "        init_nltk()\n",
    "        print('[OK] NLTK resources downloaded.')\n",
    "        if not args.data and not args.app:\n",
    "            return\n",
    "\n",
    "    if args.app:\n",
    "        launch_app(outdir)\n",
    "        return\n",
    "\n",
    "    if not args.data:\n",
    "        raise SystemExit('Provide --data path to run training/scoring.')\n",
    "\n",
    "    # Load data and map columns\n",
    "    df_raw = load_dataset(\n",
    "        args.data,\n",
    "        text_col=args.text_col,\n",
    "        rating_col=args.rating_col,\n",
    "        id_col=args.id_col,\n",
    "        category_col=args.category_col,\n",
    "        label_col=args.label_col,\n",
    "    )\n",
    "\n",
    "    # Basic cleaning & labeling\n",
    "    print(f\"[INFO] Loaded {len(df_raw)} rows. Columns: {df_raw.columns.tolist()}\")\n",
    "    df = df_raw.copy()\n",
    "    df['reviewText'] = df['reviewText'].astype(str).map(clean_text)\n",
    "\n",
    "    # Sentiment path\n",
    "    if args.engine == 'rule' or args.predict_only:\n",
    "        print('[INFO] Using rule-based sentiment (VADER/TextBlob).')\n",
    "        df_scored = predict_rule(df)\n",
    "        df_scored.to_csv(outdir / 'predictions_rule_based.csv', index=False)\n",
    "        print('[OK] Saved predictions_rule_based.csv')\n",
    "        if args.do_eda:\n",
    "            do_eda(df_scored, outdir)\n",
    "        return\n",
    "\n",
    "    # ML path (train + eval + visuals)\n",
    "    print('[INFO] Training ML model (TFIDF + LogisticRegression) ...')\n",
    "    res = train_ml(df, outdir, seed=args.seed)\n",
    "\n",
    "    eval_df = pd.DataFrame({\n",
    "        'text': res['X_valid'],\n",
    "        'label_true': res['y_valid'],\n",
    "        'label_pred': res['y_pred']\n",
    "    })\n",
    "    eval_df.to_csv(outdir / 'eval_predictions_valid.csv', index=False)\n",
    "    print('[OK] Wrote eval artifacts to', outdir)\n",
    "\n",
    "    # EDA on full dataset using model labels for consistency\n",
    "    df_for_eda = df.copy()\n",
    "    # Get labels for whole corpus (might be slow on huge datasets)\n",
    "    print('[INFO] Scoring entire corpus for EDA ...')\n",
    "    labels_all, _ = predict_ml(res['pipe'], df_for_eda['reviewText'].tolist())\n",
    "    df_for_eda['label'] = labels_all\n",
    "\n",
    "    do_eda(df_for_eda, outdir)\n",
    "    print('[OK] Visuals exported to', outdir)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7253657a-9c7b-44fd-8abd-0cb7979c8ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
